{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, train, to_tensor=True):\n",
    "        self.mnist = torchvision.datasets.MNIST(\n",
    "            \"files/mnist\", train=train, download=True\n",
    "        )\n",
    "        self.fashion_mnist = torchvision.datasets.FashionMNIST(\n",
    "            \"files/fashion_mnist\", train=train, download=True\n",
    "        )\n",
    "        self.to_tensor_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "        self.dataset = []\n",
    "        self.mnist_size = len(self.mnist)\n",
    "\n",
    "        for i in range(self.mnist_size):\n",
    "            number_img, number_label = self.mnist[i]\n",
    "            left_img, left_label = random.choice(self.fashion_mnist)\n",
    "\n",
    "            while True:\n",
    "                right_img, right_label = random.choice(self.fashion_mnist)\n",
    "                if left_label != right_label:\n",
    "                    break\n",
    "\n",
    "            img = self.__concat_images(left_img, number_img, right_img)\n",
    "            label = left_label if number_label % 2 == 0 else right_label\n",
    "\n",
    "            if to_tensor:\n",
    "                img = self.to_tensor_transform(img)\n",
    "\n",
    "            self.dataset.append((img, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.mnist_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    def __concat_images(self, left, center, right):\n",
    "        IMG_SIZE = 28\n",
    "        img = Image.new(\"L\", (IMG_SIZE * 3, IMG_SIZE))\n",
    "        img.paste(left, (0, 0))\n",
    "        img.paste(center, (IMG_SIZE, 0))\n",
    "        img.paste(right, (2 * IMG_SIZE, 0))\n",
    "        return img\n",
    "\n",
    "    @staticmethod\n",
    "    def decode_label(label):\n",
    "        mapping = {\n",
    "            0: \"T-shirt/Top\",\n",
    "            1: \"Trouser\",\n",
    "            2: \"Pullover\",\n",
    "            3: \"Dress\",\n",
    "            4: \"Coat\",\n",
    "            5: \"Sandal\",\n",
    "            6: \"Shirt\",\n",
    "            7: \"Sneaker\",\n",
    "            8: \"Bag\",\n",
    "            9: \"Ankle Boot\",\n",
    "        }\n",
    "\n",
    "        return mapping[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = CustomDataset(train=True)\n",
    "test = CustomDataset(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test[1][1])\n",
    "test[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(0, -1),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, 512)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "error = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Flatten(start_dim=0, end_dim=-1)\n",
      "    (10): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=10, bias=True)\n",
      "    (3): Softmax(dim=None)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 84])\n",
      "128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/blazej/miniconda3/envs/obrazy/lib/python3.9/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0916, 0.1112, 0.0875,  ..., 0.1162, 0.1001, 0.1171],\n",
       "        [0.0937, 0.1100, 0.0870,  ..., 0.1168, 0.0982, 0.1197],\n",
       "        [0.0920, 0.1106, 0.0887,  ..., 0.1175, 0.0999, 0.1167],\n",
       "        ...,\n",
       "        [0.0945, 0.1086, 0.0891,  ..., 0.1191, 0.0983, 0.1168],\n",
       "        [0.0942, 0.1115, 0.0836,  ..., 0.1164, 0.1015, 0.1147],\n",
       "        [0.0938, 0.1104, 0.0872,  ..., 0.1181, 0.0984, 0.1175]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(images.shape)\n",
    "    print(len(labels))\n",
    "    break\n",
    "\n",
    "model(images[:, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training Loss: 2.149119\n",
      "Epoch 2 Training Loss: 2.049256\n",
      "Epoch 3 Training Loss: 2.008715\n",
      "Epoch 4 Training Loss: 1.903554\n",
      "Epoch 5 Training Loss: 1.817834\n",
      "Epoch 6 Training Loss: 1.777758\n",
      "Epoch 7 Training Loss: 1.756065\n",
      "Epoch 8 Training Loss: 1.739413\n",
      "Epoch 9 Training Loss: 1.731407\n",
      "Epoch 10 Training Loss: 1.721715\n",
      "Epoch 11 Training Loss: 1.716482\n",
      "Epoch 12 Training Loss: 1.711226\n",
      "Epoch 13 Training Loss: 1.707252\n",
      "Epoch 14 Training Loss: 1.702682\n",
      "Epoch 15 Training Loss: 1.696619\n"
     ]
    }
   ],
   "source": [
    "# loss_min = np.Inf\n",
    "\n",
    "model.train()\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        log_ps = model(images)\n",
    "        loss = error(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    running_loss = running_loss / len(train_loader.sampler)\n",
    "\n",
    "    print(f\"Epoch {e+1} Training Loss: {running_loss:.6f}\")\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.674518\n",
      "Test Accuracy of 0: 81.68870803662259%\n",
      "Test Accuracy of 1: 92.58130081300813%\n",
      "Test Accuracy of 2: 76.87747035573122%\n",
      "Test Accuracy of 3: 86.54618473895583%\n",
      "Test Accuracy of 4: 70.5940594059406%\n",
      "Test Accuracy of 5: 91.83477425552354%\n",
      "Test Accuracy of 6: 8.864541832669323%\n",
      "Test Accuracy of 7: 93.04174950298211%\n",
      "Test Accuracy of 8: 93.8894277400582%\n",
      "Test Accuracy of 9: 90.88960342979635%\n",
      "\n",
      "Test Accuracy (Overall): 78.64%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "class_correct = list(0. for _ in range(10))\n",
    "class_total = list(0. for _ in range(10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        output = model(images)\n",
    "        loss = error(output, labels)\n",
    "\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "\n",
    "        _, pred = torch.max(output, 1)\n",
    "\n",
    "        correct = np.squeeze(pred.eq(labels.data.view_as(pred)))\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            label = labels.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "test_loss = test_loss / len(test_loader.sampler)\n",
    "print(f\"Test Loss: {test_loss:.6f}\")\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 10:\n",
    "        print(f\"Test Accuracy of {i}: {100*class_correct[i] / class_total[i]}%\")\n",
    "    else:\n",
    "        print(f\"Test Accuracy of {i}: N/A\")\n",
    "\n",
    "print(f\"\\nTest Accuracy (Overall): {100*sum(class_correct) / sum(class_total)}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter = transforms.ToPILImage()\n",
    "\n",
    "img: Image = converter(test[0][0])\n",
    "img.width\n",
    "img.resize((40 * img.width, 40 * img.height), Image.NEAREST)\n",
    "pred = model(test[0][0])\n",
    "torch.argmax(pred, 1)\n",
    "test[0][1]\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (obrazy)",
   "language": "python",
   "name": "obrazy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
